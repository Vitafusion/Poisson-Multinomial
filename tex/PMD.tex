%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt]{article}
\usepackage{latexsym,epsfig,graphicx,epstopdf,amsmath,amssymb,amscd,pifont, multirow,chicago,psfrag,paralist,dsfont,url}
\usepackage[titletoc]{appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textwidth  6.6in \textheight 9.2in \topmargin -.0in \oddsidemargin
-0.0in \evensidemargin -0.0in \pagestyle{plain}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\thetavec}{{\boldsymbol{\theta}}}
\newcommand{\veps}{\varepsilon}
\newcommand{\vepsvec}{{\boldsymbol{\varepsilon}}}
\newcommand{\Sigmavec}{{\boldsymbol{\Sigma}}}
\newcommand{\wvec}{{\boldsymbol{w}}}
\newcommand{\zerovec}{{\boldsymbol{0}}}
\newcommand{\onevec}{{\boldsymbol{1}}}
\newcommand{\Ivec}{{\boldsymbol{I}}}
\newcommand{\betavec}{{\boldsymbol{\beta}}}
\newcommand{\betahat}{{\widehat{\beta}}}
\newcommand{\etavec}{{\boldsymbol{\eta}}}
\newcommand{\thetavecC}{{\boldsymbol{\theta}_C}}
\newcommand{\thetavecT}{{\boldsymbol{\theta}_T}}
\newcommand{\thetaveca}{\thetavec_{1}}
\newcommand{\thetavecb}{\thetavec_{2}}
\newcommand{\EE}{\mathbf{E}}
\newcommand{\Bset}{\mathbf{B}}
\newcommand{\Xset}{\mathbf{X}}
\newcommand{\Sset}{\mathbf{S}}
\newcommand{\cp}{{\rm CP}}
\newcommand{\pr}{{\rm Pr}}
\newcommand{\mvn}{{\rm MVN}}
\newcommand{\mse}{{\rm MSE}}
\newcommand{\emse}{{\rm EMSE}}
\newcommand{\TAE}{{\rm TAE}}
\newcommand{\MAE}{{\rm MAE}}
\newcommand{\bin}{{\rm bin}}
\newcommand{\enum}{{\rm enum}}
\newcommand{\Var}{{\rm Var}}
\newcommand{\muhat}{\widehat{\mu}}
\newcommand{\sigmahat}{\widehat{\sigma}}
\newcommand{\thetavechat}{\widehat{\thetavec}}
\newcommand{\thetavecmis}{\thetavec_{\ast}}
\newcommand{\mumis}{\mu_{\ast}}
\newcommand{\sigmamis}{\sigma_{\ast}}
\newcommand{\thetavecahat}{\widehat{\thetavec}_1}
\newcommand{\thetavecbhat}{\widehat{\thetavec}_2}
\newcommand{\amse}{{\rm AMSE}}
\newcommand{\avar}{{\rm AVar}}
\newcommand{\abias}{{\rm ABias}}
\newcommand{\bias}{{\rm Bias}}
%\newcommand{\diag}{{\rm Diag}}
\newcommand{\Arg}{{\rm Arg}}
\newcommand{\Ber}{{\rm Ber}}
\newcommand{\atantwo}{{\rm atan2}}
\newcommand{\ivec}{{\boldsymbol{i}}}
\newcommand{\dgoto}{\overset{d}{\rightarrow}}
\newcommand{\Pgoto}{\overset{P}{\rightarrow}}
\newcommand{\asgoto}{\overset{a.s.}{\longrightarrow}}
\newcommand{\sev}{\textrm{sev}}
\newcommand{\nor}{\textrm{nor}}
\newcommand{\N}{\textrm{N}}
\newcommand{\diag}{\textrm{diag}}
\newcommand{\PMD}{\textrm{PMD}}
\newcommand{\wh}{\widehat}
\newcommand{\sigmaR}{\sigma_{R}}
\newcommand{\muR}{\mu_{R}}
\newtheorem{result}{Result}
\newcommand{\Xvec}{\boldsymbol{X}}
\newcommand{\Zvec}{\boldsymbol{Z}}
\newcommand{\xvec}{\boldsymbol{x}}
\newcommand{\kvec}{\boldsymbol{k}}
\newcommand{\rvec}{\boldsymbol{r}}
\newcommand{\pvec}{\boldsymbol{p}}
\newcommand{\minitab}[2][l]{\begin{tabular}{#1}#2\end{tabular}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\baselinestretch{1.25}
\renewcommand{\arraystretch}{.8}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[ruled,vlined]{algorithm2e}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\theoremstyle{definition}
%\newtheorem{exmp}{Example}[section]



\newtheorem{example}{Example}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{defn}{Definition}
\newtheorem{corl}{Corollary}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textwidth  6.6in \textheight 9.2in \topmargin -.5in \oddsidemargin
-0.0in \evensidemargin -0.0in \pagestyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\baselinestretch{1.25}
\renewcommand{\arraystretch}{.8}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{tocdepth}{2}

%-------------------------------------------------------------------------
\begin{document}
%%%%%%%%%%%%TITLE%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\title{The Computing of Probability Mass Functions for the Poisson Multinomial Distribution}

\title{The Poisson Multinomial Distribution and Its Applications in Voting Theory, Ecological Inference, and Machine Learning}


%\iffalse
\author{
Zhengzhi Lin, Yueyao Wang, and Yili Hong\\[1.5ex]
{Department of Statistics, Virginia Tech, Blacksburg, VA 24061}
}
%\fi
	
\date{\today}
	
\maketitle
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The Poisson Multinomial Distribution (PMD) is the sum of $n$ independent indicators, in which each indicator is an $m$ element vector that follows a multinomial distribution. The PMD is useful in many areas such as, machine learning, uncertainty quantification, and voting theory. The distribution function (e.g., the probability mass function) has been studied by many authors for a long time, but there is no general computing method available for its distribution functions. In this paper, we develop algorithms to compute the probability mass function for the PMD, and we develop an R package that can calculate the probability mass function efficiently. We also study the accuracy of different methods. We illustrate the use of the PMD with three applications from voting theory, ecological inference, machine learning. We also provide examples to demonstrate the use of the R package.
		
\textbf{Key Words:} Binomial distribution; Classification; Poisson Binomial Distribution; Machine Learning; Uncertainty Quantification;  Voting Theory
\end{abstract}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\tableofcontents
\newpage
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\subsection{Motivation}

The Poisson Multinomial Distribution (PMD) is defined as sum of different independent multinomial distributions. It has applications in game theory (\citeNP{Cheng2017PlayingAG}), digital imaging (\citeNP{akter2019double}), machine learning (\citeNP{kamath2014learning}).  One of the popular areas that involved PMD is voting theory. A very simple example is, suppose we have $n$ independent voters to vote for a president from $m$ candidates, each voter has its own chance to vote on a certain candidate. It is not our concern how voters vote, we care only the voting result, that is how many votes each candidate gets. An $(n,m)$ PMD is a perfect model for this example. In machine learning, especially in classification context, when we classify $n$ samples one by one into $m$ categories, then the total number of samples assigned to each category follows a PMD. When $(n,m)$ is small it is possible to calculate the distribution function by using enumeration method. However when $n,m$ get large, we need better method, or algorithm to calculate it.




\subsection{Related Literature and Contribution of This Work}

Ecological inference

https://www.pnas.org/content/96/19/10578


Some former studies have uncover PMD's structure and properties, \citeN{diakonikolas2016fourier} shows the Fourier transformation of PMD is sparse and provide a theorem that there exist algorithms to calculate PMD's density. \citeN{Daskalakis2015OnTS} also prove us PMD is $\epsilon$-cover and Central Limit Theory is valid for PMD. Other papers such as \citeN{akter2019double} shows us some interesting application of PMD and its sparsity property. Due to the huge practical value of PMD, computing its pmf is of great importance. However, there is no available algorithm for computing pmf for PMD. In this case, we are motivated to develop a thorough method to calculate its pmf.


We design three methods to calculate it,

MD-DFT(base on multi-dimensional discrete Fourier transform(MD-DFT)) method: we use Multidimensional Fast Discrete Fourier Transformation algorithm to calculate PMD's exact probability mass function.

Simulation based method: we use multinomial distributions to simulate the process of voting and use the frequency as our result to approximate true result.

Normal approximation based method: when $n$ is large, we apply CLT and use a approximated normal distribution as our result.


We also compare the time efficiency and accuracy of each method, and come up a guide of the best situation for each method. A R package is developed to implement these three methods, users can call certain function to calculate user given $(n,m)$ PMD pmf's.




\subsection{Overview}
The rest of the paper is organized as follows. \\
\\
In the coming part, we describe the definition of Poisson Multinomial distribution as well as demonstrate some useful properties. In the third part, we explain three ways of computing the probability mass function by details.\\
\\
The fourth part of this article compare the accuracy and time efficiency of the listed three methods. \\
\\
In the application part, we begin with a simple scenario of committee voting where small dimensional PMDs can be applied. Later we show a way of implementing medium size PMDs to AI4I 2020 Predictive Maintenance Dataset which is a large scale aggregated dataet(large $n$). We also place PMD in a classification scenario using the solar cells EI image data.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Poisson Multinomial Distribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Definition of the Distribution}
		
Let $(I_{i1}, \ldots, I_{i,m})'$, $i = 1,\dots,n$ be a sequence of independent indicator vectors where exactly one of $(m)$ categories successes. That is there is one and only one of those $I_{ij}$, $j=1,\ldots, m$ can take value one for each $i$. Take election as an example, suppose there are several candidates, denoted as $j = 1,\dots,m$, and some voters, voters are independent to each other, denoted as $i = 1,\dots,n$, every voter can only vote for one candidate, if the $i$th voter votes for $j$th candidate, then $I_{ij} = 1$, else $I_{ij} = 0$.  Denote $p_{ij} = \Pr(I_{ij} = 1)$, as the success probability of $j$th candidate gets a vote from $i$th voter. The Poisson multinomial random variable $(X_1, \dots, X_m)' $ is defined as the sum of $n$ independent and non-identical distributed indicator vectors $(I_{i1}, \ldots, I_{i,m})'$, which is the result of election, the total votes each candidate gets. Here $X_j=\sum_{i=1}^{n}I_{ij}$. Assume all probabilities $p_{ij}$'s are known, we care about the distribution of our election result, which is the probability of a certain result. Because of the sum constraints that, $\sum_{j=1}^{m} I_{ij}= 1$, $\sum_{j=1}^{m} p_{ij}= 1$, and $\sum_{j=1}^{m} X_{j}= n$, one can drop the last random variable when the focus is on the distribution of $(X_1, \ldots, X_m)'$. That is we will focus on the random vectors $\boldsymbol{I}_i=(I_{i1}, \ldots, I_{im})'$ and $\boldsymbol{X}=(X_1, \ldots, X_m)'$. The distribution of the $\boldsymbol{X}$ is called the Poisson multinomial distribution (PMD), which is denoted by
$$\boldsymbol{X}  = \sum_{i = 1}^n \boldsymbol{I}_i \sim \PMD(n,m,\boldsymbol{P}_{n\times m}),$$
where the success probability matrix (SPM) is
\begin{equation*}
\boldsymbol{P}_{n \times m} = \begin{pmatrix}
p_{11} &  \dots & p_{1m} \\
\vdots & \ddots & \vdots \\
p_{n1} &  \dots & p_{nm} \\
\end{pmatrix}.
\end{equation*}
Let vector $\boldsymbol{x} = (x_1,\dots,x_m)'$, the probability mass function (pmf) of PMD,
$$p(\boldsymbol{X}=\boldsymbol{x}) = \text{Pr} \left( X_1 = x_1, \dots, X_m = x_{m-1}, X_{m} = n-\sum_{i=1}^{m}x_i \right)$$
is of interest. Moreover, in the next section, we only concern $p(x_1, \dots, x_{m-1})$ because $x_{m} = n-\sum_{i=1}^{m-1}x_i $. Consequently , $(x_{1},\dots,x_m)'$, and $(x_{1},\dots,x_{m-1})'$ are equivalent, we can also denote the latter as $\boldsymbol{x}$ for convenience. Correspondingly, we can also denote $(X_1,\dots,\X_{m-1})$ as $\boldsymbol{X}$ due to the equivalence of  $(X_1,\dots,X_m)$ and $(X_1,\dots,X_{m-1})$. Note this will be  important in the $3^{\textbf{rd}}$ part of this paper.\\
\\
Let's see a simple example, we have three candidates and four voters, and we have
\begin{equation*}
\boldsymbol{P}_{4 \times 3} = \begin{pmatrix}
0.1 &  0.2 & 0.7\\
0.5 & 0.2 & 0.3\\
0.4 &  0.5 & 0.1\\
0.8 & 0.1 & 0.1
\end{pmatrix}.
\end{equation*}
We want to know the probability of a result that candidate 1 gets 4 votes and others  gets 0 vote that is, $\boldsymbol{x} =  (4,0,0)'$. 
\begin{equation*}
P\{\boldsymbol{X} = \boldsymbol{x}\} = 0.1\times 0.5 \times 0.4 \times 0.8 = 0.016
\end{equation*}
The probability of $\boldsymbol{X}=(1,3,0)'$ is
 \begin{multline*}
 P\{\boldsymbol{X} = (1,3,0)\} = 0.1\times 0.2 \times 0.5 \times 0.1\\ +
 0.5\times0.2\times0.5 \times 0.1 + 0.4\times0.2\times0.2\times0.1 + 0.8\times0.2\times0.2\times0.5 = 0.0236
 \end{multline*}
 Keep doing this we can calculate all possible outcomes.\\
 \\

Note that when the SPM is identical across all rows, that is, $I_{i}$, $i = 1, \dots n$ are identically distributed, the distribution of $X$ can be simplified as multinomial distribution. Hence, the PMD is a generalization of the multinomial distribution. When $m=1$, the PMD is reduced to the Poisson binomial distribution as in \citeN{hong2013computing}.

In related literature, \citeN{hong2013computing} consider the exact and approximate methods for computing the pmf of the Poisson binomial distribution. \citeN{zhang2018generalized} introduce the general Poisson binomial distribution and develop an algorithm to compute its distribution functions.


\subsection{Properties of the Distribution}

\begin{thm}
Given random variable $\boldsymbol{X}$ that follows a Poisson-Multinomial distribution with $\boldsymbol{P}_{n\times m}$, the mean of $\boldsymbol{X}$ is 
   $\boldsymbol{\mu} = \left( p_{\cdot1} ,\dots,p_{\cdot,m-1},p_{\cdot m}\right)'$, where $p_{\cdot k} = \sum_{i=1}^{n}p_{i k}$. \\
The variance-covariance matrix of $\boldsymbol{X}$ is a $m \times m$ matrix $\boldsymbol{\Sigma}$ that has entries $\Sigma_{ij}$ defined as
\begin{equation*}
   \Sigma_{ij} = 
           \begin{cases}
             \sum_{k=1}^{n}p_{ki}(1-p_{ki}) & \quad \text{if } i=j\\
             -\sum_{k=1}^{n}p_{ki}p_{kj} & \quad \text{if } i \neq j\\
           \end{cases}
\end{equation*}

The CF for the PMD is 
\begin{equation*}
\phi_{\boldsymbol{X}}(t_1, \dots, t_{m-1}) = \phi_{(X_1,\dots,X_m)'}(t_1, \dots, t_{m-1})  =  \sum_{x_1 = 0}^{n}\cdots \sum_{x_{m-1} = 0}^n p(x_1,\ldots,x_{m-1})\exp\left(\ivec\sum_{j=1}^{m-1}t_jx_j\right).
\end{equation*}
where  $\ivec=\sqrt{-1}$.
\end{thm}
The derivations of the above properties are trivial. One important thing is that we realize the covariance matrix $\boldsymbol{\Sigma}$ is singular, but we still can get a non-singular $n \times (m-1)$ matrix, say $\boldsymbol{\Sigma}_{0}$, to given $P_{n \times m}$ by deleting the last column of $\boldsymbol{\Sigma}$ due to the fact that the last column is linear dependent on the first $m-1$ columns. Similarly, the last element of the expectation vector of $X$ is also redundant since it equals to $n$ minus summation of the first $m-1$ elements. Therefore we can denote $\EE(X)$ by replacing $\boldsymbol{\mu}$ to $\boldsymbol{\mu}_0 = \left( p_{\cdot1} ,\dots,p_{\cdot,m-1}\right)'$.


\begin{thm}
If the SPM $\boldsymbol{P}$ can be written as a combination of diagonal matrices $\boldsymbol{P}_1, \boldsymbol{P}_2, \dots, \boldsymbol{P}_{K}$, $\boldsymbol{P} = \textbf{\diag}(\boldsymbol{P}_{k}), k=1,\dots,K$. Then the outcome of $\boldsymbol{P}$, $\boldsymbol{x}$ can hereby be decomposed in to  outcomes of the diagonal matrices, $\boldsymbol{x} = (\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{K})$. The pmf can computed as the product of the corresponding marginal pmfs. That is
\begin{align*}
p(\boldsymbol{x}|\boldsymbol{P})= p(\boldsymbol{x}_{1} \cap \dots \cap \boldsymbol{x}_{K}|\boldsymbol{P}_1,\dots,\boldsymbol{P}_{K} )= \prod_{k=1}^K p(\boldsymbol{x}_{k}|\boldsymbol{P}_{k}).
\end{align*}
\end{thm}
To show this, we assume $\boldsymbol{P}$ is $n \times m$ and $\boldsymbol{P}_{k}$ is $n_k \times m_k$. $\sum_{k=1}^K n_k = n$ and $\sum_{k=1}^K m_k = m$. To demonstrate the situation here, suppose there are $n$ voters to vote $m$ candidates, certain groups of voters only vote for certain groups of candidates and there are no overlaps . Heuristically, we can separate candidates and voters into independent groups, in each group $k$, $k = 1,\dots,K$, voters voting for corresponding candidates 
described by probability matrix $\boldsymbol{P}_k$. Strict proof can be done by decomposition of characteristic function of $\boldsymbol{P}$ into characteristic functions of $\boldsymbol{P}_{k}$s.



\section{Computation of The Probability Mass Function}\label{sec:CA.driving.study}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We introduce three methods for computing the pmf, which are the method based on multidimensional discrete Fourier transform (MD-DFT), the normal approximation (NA) method, and simulation based method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The MD-DFT Method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section we provide an exact formula to compute the pmf of the PMD. The formula is based on the characteristic function (CF) of the PMD and the MD-DFT.

Multidimensional Fourier Transform


The CF of $\boldsymbol{X}=(X_1, \dots, X_{m-1})'$ is
\begin{align*}
\phi(t_1, \dots, t_{m-1}) & = \EE\left[\exp\left(\ivec\sum_{j=1}^{m-1}t_jX_j\right)\right]=\EE\left[\exp\left(\ivec\sum_{i = 1}^n \sum_{j=1}^{m-1}t_j I_{ij}\right)\right].
\end{align*}
Here $\ivec=\sqrt{-1}$. We notice\\
\begin{equation*}
\begin{split}
  &\EE\left[\exp\left(\ivec\sum_{j=1}^{m-1}t_jX_j\right)\right] = \sum_{x_1 = 0}^{n}\cdots \sum_{x_{m-1} = 0}^n p(x_1,\ldots,x_{m-1})\exp\left(\ivec\sum_{j=1}^{m-1}t_jx_j\right).\\
  \\
  &\EE\left[\exp\left(\ivec\sum_{i = 1}^n \sum_{j=1}^{m-1}t_j I_{ij}\right)\right] = \EE\left[ \exp\left( \ivec\sum_{j=1}^{m-1} t_jI_{1j} + \dots + \ivec\sum_{j=1}^{m-1} t_jI_{nj}\right)\right].\\
  \\
  & = \prod_{i=1}^n \EE\left[ \exp\left( \ivec \sum_{j=1}^{m-1} t_j I_{ij}\right)\right] = \prod_{i=1}^n \left[(1 - \sum_{j=1}^{m-1}p_{ij})+\sum_{j=1}^{m-1}p_{ij}\exp(\ivec t_j)\right].
\end{split}
\end{equation*}
Therefore we get
\begin{align*}
\sum_{x_1 = 0}^{n}\cdots \sum_{x_{m-1} = 0}^n p(x_1,\ldots,x_{m-1})\exp\left(\ivec\sum_{j=1}^{m-1}t_jx_j\right)= \prod_{i=1}^{n}\left[(1 - \sum_{j=1}^{m-1}p_{ij})+\sum_{j=1}^{m-1}p_{ij}\exp(\ivec t_j)\right].
\end{align*}
Let $t_j = \omega l_j$, $l_j = 0, \ldots, n$, $\omega = 2\pi/(n+1)$. Then the equation becomes
\begin{align}
\frac{1}{(n+1)^{m-1}} \sum_{x_1 = 0}^{n}\cdots \sum_{x_{m-1} = 0}^n p(x_1,\ldots,x_{m-1}) \exp\left(\ivec\omega\sum_{j=1}^{m-1}l_j x_j\right)= \frac{1}{(n+1)^{m-1}} q(l_1, \ldots, l_{m-1}),
\end{align}
where
$$ q(l_1, \ldots, l_{m-1})=\prod_{i=1}^{n}\left[(1 - \sum_{j=1}^{m-1}p_{ij})+\sum_{j=1}^{m-1}p_{ij}\exp(\ivec \omega l_j)\right].$$	
Note that $q(l_1, \ldots, l_{m-1})$ can be computed directly. The left side of equation (1) is the inverse multi-dimensional discrete Fourier transform of the sequence $ p(x_1,\ldots,x_{m-1}), x_i = 0 , \dots, n$. Therefore we can apply MD-DFT on both sides to recover the sequence, we obtain the pmf as
\begin{equation}
p(x_1, \ldots, x_{m-1}) = \frac{1}{(n+1)^{m-1}}\sum_{l_1 = 0}^{n}\cdots \sum_{l_{m-1} = 0}^n q(l_1, \ldots, l_{m-1}) \exp\left(-\ivec\omega\sum_{j=1}^{m-1}l_j x_j\right)
\end{equation}
Let $\ell = (l_1,\dots,l_{m-1})$, then we will have $(n
+1)^{m-1}$ different $\ell$ as $l_i$ values from $0$ to $n$. For example, if we have $n=4$, $m=4$, then $\ell$ can be $(0, 0, 0), (0, 0, 1), \dots, (4, 4, 4)$, 125 different vectors in total. Now we have $q(l_1,\dots,l_{m-1}) = q(\ell)$. We design to use these vectors to generate respective $p(x_1,\dots,x_{m-1})$. For each $\ell$, we get a $p(x_1,\dots,x_{m-1})$.

To get all $p(x_1,\dots,x_{m-1})$ with respect to given $n$, $m-1$ and $P_{n \times (m-1)}$, we apply the fast Fourier transformation (FFT) algorithm  from GSL Scientific Library to make calculation efficient. This FFT algorithm is C language based, we implemented it and wrote a new MD-DFT algorithm. Our MD-DFT algorithm can calculate all values of distribution function as long as we input our $P_{n\times (m-1)}$ matrix, and it can be called from R.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Normal-Approximation Based Method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Let $\pvec_i$ be the $i$th row of the SPM $P$. Let $\pvec=\sum_{i=1}^n\pvec_i/n$ be the average of the rows of $\boldsymbol{P}$.


\begin{thm}
For a Poisson-Multinomial random variable $\boldsymbol{X} = (X_1,\dots,X_{m-1})'$ with mean $\boldsymbol{\mu}_0$ and non-singular covariance matrix $\boldsymbol{\Sigma}_0$. For each outcome $\boldsymbol{\boldsymbol{x}}_i$, and its neighbourhood interval $\mathcal{N}_{\boldsymbol{\boldsymbol{x}}_i} = [\boldsymbol{\boldsymbol{x}}_i-0.5,\boldsymbol{\boldsymbol{x}}_i+0.5]$. There exists a non-singular matrix $\boldsymbol{C}$ such that $\boldsymbol{\Sigma}_0 = CC'$ and the error bound of Central Limit Theory approximation is
\begin{equation*}
    |P(\boldsymbol{X} \in \mathcal{N}_{\boldsymbol{x}_i}) - P(\boldsymbol{Z} \in \mathcal{N}_{\boldsymbol{x}_i-\boldsymbol{\mu}_0})| \leq b (m-1)^{\frac{1}{4}} \sum_{i=1}^{n}\EE|C^{-1}\boldsymbol{I}_{i}|^3
\end{equation*}
where $\boldsymbol{Z}$ is normal with mean 0 and covariance matrix $\boldsymbol{\Sigma}_0$.
\end{thm}
We first show that CLT can be applied to
\begin{equation*}
\frac{\boldsymbol{X}}{n} - \pvec = \frac{(X_1,\dots,X_{m-1})}{n} - \pvec = \frac{(X_1 - \sum_{i=1}^{n}p_{i1},\dots,X_{m-1} - \sum_{i=1}^{n}p_{i,m-1})}{n}
\end{equation*}
We just need to show that for any $j = 1,\dots,m-1$, $X_j - \sum_{i=1}^{n}p_{ij}$ satisfies conditions of CLT. Where $X_{j} = \sum_i I_{ij}$ \\
Notice that for any $\delta > 0$
\begin{equation*}
    1\geq p_{ij}(1-p_{ij}) = \Var (I_{ij}) \geq E(|I_{ij}|^{2+ \delta})
\end{equation*}
Therefore, let $s_n^2 = \sum_{i=1}^{n}\Var(I_{ij})$, we have
\begin{equation*}
    \frac{1}{s_n^{2+\delta}}\sum_{i=1}^{n}E|I_{ij}|^{2 + \delta} \leq  \frac{1}{s_n^{2+\delta}}\sum_{i=1}^{n}\Var(I_{ij}) = \frac{1}{s_n^\delta}
\end{equation*}
As $n \rightarrow \infty$, $s_n \rightarrow \infty$, so the above equation converges to 0. Therefore $X_j$ satisfies Lyapunov condition, thus CLT can be applied to $X_j$.\\
To compute the covariance matrix for $\boldsymbol{X}$,
observe that for any fix $i$, $I_{ij}$ and $I_{ik}$ has covariance $-p_{ij}p_{ik}$. Therefore, it is trivial to get the covariance matrix $$\boldsymbol{\Sigma}=\sum_{i=1}^n[\diag(\pvec_i)-\pvec_i\pvec_i']$$
By central limit theorem (CLT),
$$\left(\frac{\boldsymbol{X}}{n}-\pvec\right)\dot\sim \N\left(\zerovec, \frac{1}{n}\boldsymbol{\Sigma}\right).$$\\
To show $\mathbf{Theorem 3}$, we notice
\begin{align*}
    \boldsymbol{X} = (X_1,\dots,X_{m-1})' = \sum_{i=1}^{n} \boldsymbol{I}_{i} 
\end{align*}
Easy to get $\boldsymbol{I}_i - \EE \boldsymbol{I}_i$ has mean 0. Hereby $\boldsymbol{X} - \EE \boldsymbol{X} = \sum (\boldsymbol{I}_i - \EE \boldsymbol{I}_i)$ also has mean 0. The covariance of $\boldsymbol{X}$ is $\boldsymbol{\Sigma} = CC'$.\\ 
By extended $\textbf{Berry-Esseen theory}$ (V. Yu. Bentkus, A Lyapunov-type bound in Rd
), there existing  a constant $b$ such that 
\begin{equation*}
    |P\left(\boldsymbol{X} \in \mathcal{N}_{\boldsymbol{x}_i}\right) - P\left(\boldsymbol{Z} \in \mathcal{N}_{\boldsymbol{x}_i-\boldsymbol{\mu}_0} \right)| \leq c(m-1)^{\frac{1}{4}}\sum_{i=1}^{n} \EE \left|C^{-1}\boldsymbol{I}_i\right|^3
\end{equation*}
Where $\boldsymbol{Z}$ is $\textbf{CLT}$ multivariate normal distribution of with 0 mean and covariance $\boldsymbol{\Sigma}_0$.\\ When $n$ is sufficiently large, our R package use the normal distribution to calculate our probability by demand of users.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Simulation-Based Method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
One can simulate $I_i$ from multinomial distribution and then compute $X$. Repeat this many times to generate enough samples for $X$. Then use the sample distribution to approximate the true distribution.
To be specific,
\begin{enumerate}[Step 1]
    \item randomly generate $I_i$ with given $p_i$ using multinomial distribution.
    \item repeat step 1 to generate $I_1,\dots,I_n$. Calculate $X = (X_1,\dots,X_{m})'$, where $X_j = \sum_{i=1}^{n}I_{ij}, j=1,\dots,m$.
    \item repeat step 1 and step 2 for $T$ times, and calculate the frequency of $X$ as to form our distribution density.
\end{enumerate}
\begin{example}
Suppose we are given $n=3$, $m=2$
\begin{equation*}
P_{n \times m} = P_{3 \times 1} = \begin{pmatrix}
0.1 & 0.9\\
0.5 & 0.5\\
0.3 & 0.7\\
\end{pmatrix}.
\end{equation*}
We first generate $I_1$ by using multinomial distribution with respect to probability vector $(0.1,0.9)$, we get $I_1 = (0,1)$  ,then we generate $I_2$ using multinomial distribution with probability vector $(0.5,0.5) \rightarrow I_2 = (1,0)$, and also generate $I_3$ by using prob vector $(0.3,0.7) \rightarrow I_3 = (0,1)$. Now we have a simulation result $X = I_1+I_2+I_3 = (1,2)$. We repeat above process $10^4$ times to get $10^4$ results, finally if we want to know probability of (1,2), we can calculate the frequency of it in the $10^4$ results.
\end{example}
We design a C based algorithm to perform this simulation process. Our algorithm automatically calculate probabilities of all possible results and it can be called from R.\\

In our package, as $n$ and $m$ get large, the total number of probability mass points $(n+1)^{m-1}$ will be extensively great. For time efficiency purpose, we only calculate the probability as customer demand. User will be asked to input the resulting vector, and the probability for the vector will be calculated.




\begin{thm}
Given repeating time \mbox{B} and $n \times m$ matrix $\boldsymbol{P}$, there will be totally  $N=\binom{n+m-1}{m-1}$ different results, denote them as  $\boldsymbol{x}_1,\dots,\boldsymbol{x}_m$. Let the probability for a specific result $\boldsymbol{x}_{i}$ be $p_{\boldsymbol{x}_{i}}$, $i=1,\dots,N$. The estimate of  $p_{\boldsymbol{x}_{i}}$ using simulation method is $\hat{p}_{\boldsymbol{x}_i}$. We have the following expected error given by Central Limit Theory,
\begin{equation*}
    \EE|p_{\boldsymbol{x}_i} - \hat{p}_{\boldsymbol{x}_i}| =  \sqrt{\frac{2 p_{\boldsymbol{x}_i}(1-p_{\boldsymbol{x}_i})}{\pi B}}
\end{equation*}
The expected total absolute error(expected \mbox{TAE}) will be smaller or equal to 
$\sqrt{\frac{2(N-1)}{\pi B}}$.
\end{thm}
For any result $\boldsymbol{x}_i,i=1,\dots,N$. Consider a Bernoulli $\textbf{r.v}$ $Y_i$ with probability $p_{\boldsymbol{x}_i}$ to be 1, and $1-p_{\boldsymbol{x}_i}$ to be 0. By repeating the trail for $B$ times, we get $\textbf{i.i.d}$ random variables $Y_1,\dots,Y_{B}$. By $\textbf{WLLN}$
\begin{equation*}
    \Bar{Y}-p_{\boldsymbol{x}_i} \xrightarrow{d} N(0,\sigma^2)
\end{equation*}
where $\sigma^2 = \frac{p_{\boldsymbol{x}_i}(1-p_{\boldsymbol{x}_i})}{B}$. Trivally, we get the expectation of absolute error for a single $\boldsymbol{x}_i$
\begin{equation*}
    \EE |\Bar{Y}-p_{\boldsymbol{x}_i}| = \frac{\sqrt{2}}{\sqrt{\pi}}\sigma = \sqrt{\frac{2}{\pi B}p_{\boldsymbol{x}_i}(1-p_{\boldsymbol{x}_i})}
\end{equation*}
Let $c = \sqrt{\frac{2}{\pi B}}$ for simplicity, then
\begin{align*}
    & \sum_{i=1}^{N}\EE |\Bar{Y}-p_{\boldsymbol{x}_i}| = c \sum_{i=1}^{N} \sqrt{p_{\boldsymbol{x}_i}(1-p_{\boldsymbol{x}_i})}  = c N \sum_{i=1}^{N}\frac{\sqrt{p_{\boldsymbol{x}_i}(1-p_{\boldsymbol{x}_i})}}{N} \\
    & \leq c N \sqrt{\sum_{i}p_{\boldsymbol{x}_i}(1-p_{\boldsymbol{x}_i})/N} = c \sqrt{N} \sqrt{1-\sum p_{\boldsymbol{x}_{i}}^2}\\ &\leq c \sqrt{N} \sqrt{1-1/N} = c\sqrt{N-1}\\
    & = \sqrt{\frac{2(N-1)}{\pi B}}
\end{align*}
The equality will be achieved if $p_{\boldsymbol{x}_1} = \dots = p_{\boldsymbol{x}_N}$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method Comparisons}\label{sec:Method Comparisons}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we compare the three methods in terms of numerical accuracy and the time efficiency, we also give out recommendations of under what condition which method will be preferred. At the first three subsections, we show the accuracy of the methods we mention earlier. Here the major accuracy criterion we use is maximum absolute error(MAE). Suppose we have an PMD matrix with $n\times m$ dimension. Hence there will be $(n+1)^{m-1}$ probability mass points notated as a set $X = \left\{x_1,\dots,x_{N}\right\}$, where $N=(n+1)^{m-1}$. By this way, MAE is defined as following,
\begin{center}
$\mathrm{MAE} = \underset{X}{\max}|p(x) - p_{\text{true}}(x)|$
\end{center}
which is the max value of the differences between probability densities calculated by our methods and true ones.
We also use total absolute error (TAE) as a supplemental criterion where
\begin{equation*}
    \mbox{TAE} = \sum_X |p(x) - p_{\text{true}}(x)|
\end{equation*}
For MD-DFT method, we test its accuracy by using small PMD matrix with given inputs due to the complexity of calculation. For other methods, let the densities calculated by MD-DFT to be the true densities and compare them with our goal method. The PMD matrices we use here are generated randomly.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Accuracy of MD-DFT Method}
As we know already, MD-DFT is a analytic proved method. For large $n$ and $m=2$, which is the Binomial Possion Distribution, the accuracy is justified by \citeN{HONG201341}. Thus for convenience and incapability of calculating the true density of large $m$, here we show the accuracy of MD-DFT by using given small $(n,m)$ pairs. For those pairs, we can work out their probability densities by hand, and compare them with the results computed by MD-DFT method.
\begin{table}%[h!]
\centering
\caption{Accuracy of MD-DFT method.}\label{tab:my_label}
\vspace{1ex}
\begin{tabular}{c|c|c|c}
\hline\hline
     $(n,m)$ & $\min (p_{i j})$ & $\max (p_{i j})$ & $\mathrm{MAE}$ \\
\hline
    (2,2) & 0.14 & 0.86 & $\leq 10^{-16}$\\
\hline
    (4,3) &0.09 &0.56 & $\leq 10^{-16}$\\
\hline
    (5,3) &0.02 &0.6 &$\leq 10^{-16}$\\
\hline
    (6,3) &0.1 &0.59 &$\leq 10^{-16}$\\
\hline
    (4,4) &0.08 &0.41 &$\leq 10^{-16}$\\
\hline
    (5,4) &0.005 &0.39 &$\leq 10^{-16}$\\
\hline
    (6,4) &0.007 &0.44 &$\leq 10^{-16}$\\
\hline\hline
\end{tabular}

\end{table}
As we see from Table 1, the $\mathrm{MAE}$ are all less or equal to machine epsilon. This shows us that MD-DFT is accurate enough and the error is only due to the computation limit of devices. Therefore, it is convincing to use MD-DFT as true probability densities in the following subsections.

\subsection{Accuracy of Normal Approximation}
Figure 1 lists all $(n,m)$ pairs we will test in this section and next section, every $(n,m)$ combination will be tested if it is covered in blue bars. The respecting bar height for $m=3,4,5,6,7,8$ is $100,83,43,25,16,17$. We use probability mass points computed by MD-DFT as true probabilities and compare those to the ones approximated by normal distribution(NA) to compute MAE and  TAE. Each PMD matrix is generated randomly using uniform distribution, thus we must consider the variation caused by randomness. To reduce the randomness effect, for each $(n,m)$ pair, we generate N PMD matrices and compute all of their MAE. Consequently, we be able to get the average MAE of them and use it as our final MAE. We use the same scheme for TAE.
\begin{figure}%[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{nm_pairs.pdf}
    \caption{(n,m) pairs, y axis value represents n. For instance, when m=3, we test the pairs from $(n=1,m=3)$ to $(n=100,m=3)$, the second bar means we testing from $(n=1,m=4)$ to $(n=83,m=4)$}
\end{figure}\\
Figure 2 shows the accuracy test results for normal approximation(NA). The x axis is n and different colors represent different m values. As n increasing, MAE decreases for the same m value. We should also consider the effect of sparsity. As $n$ and $m$ raise, there will be a boost of number of probability mass points. Hence every probability mass point will be given less value averagely. Therefore, no matter what approximation method we apply here, the MAE will decrease as the number of probability mass points grows.

\begin{figure}%[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{normal_accuracy.pdf}
    \caption{Different colors and types of lines representing different $m$. The TAE is under log transformation}
\end{figure}


\subsubsection{Accuracy of Simulation Method}
While NA and MD-DFT can cover most cases, we introduce an other method for approximating PMDs', the simulation approximation method(SA). The idea behind this method is simple, as we know a PMD is a generalized multinomial distribution. Suppose each row of PMD matrix is a probability vector of a multinomial distribution, thus a $n\times m$ PMD matrix can be seen as n multinomial distributions with respect to n m-category probability vectors. Then we can have the following algorithm to approximate each probability mass point of PMD.

\begin{algorithm}%[H]
\SetAlgoLined
\KwResult{Write here the result }
 Set the repeating time equals to $N$\;
 \For{$t=1$ \KwTo $N$}{
  \For{$i=1$ \KwTo $n$}{
    generate 1 multinomial sample wrt probability vector = $i^{th}$ row of $P_{n \times m}$
  }
  sum up all samples generated in last loop and record the result as the $t^{th}$ row of matrix R.
 }
 \begin{itemize}
     \item compute the frequency of every result that stored in R as our probability mass point value.\\
     \item compare density calculated above and computed by MD-DFT to get MAE and TAE.
 \end{itemize}
 \caption{Simulation approximation algorithm (SA)}
\end{algorithm}

We also calculated the error bound of the simulation method.
Suppose for $i$th repeat, we consider random variable matrix $[X_{1i},X_{2i},\dots,X_{n i}]'$, each $X$ is a vector of length $m$ representing a voter's voting result. We repeat $R$ times to simulate the voting process to get $X_{j1},\dots,X_{j R}$ for each $j=1,\dots, n$, then
\begin{align*}
        &U_1 = \frac{\sum_{i=1}^{R} X_{1i}}{R} \longrightarrow N\left(p_1,\frac{1}{R}\left(Diag(p_1) - p_1 p_1'\right)\right)\\
        &U_2 = \frac{\sum_{i=1}^{R} X_{2i}}{R} \longrightarrow N\left(p_2,\frac{1}{R}\left(Diag(p_2) - p_2 p_2'\right)\right)\\
        & \vdots\\
        &U_n = \frac{\sum_{i=1}^{R} X_{ni}}{R} \longrightarrow N\left(p_n,\frac{1}{R}\left(Diag(p_n) - p_n p_n'\right)\right)
\end{align*}
Therefore
\begin{align*}
    U_1 + \dots + U_n \longrightarrow N\left(\sum_{i=1}^{n}p_i, \frac{1}{R}\sum_{i=1}^{n}\left(Diag(p_i) - p_i p_i'\right) \right)
\end{align*}
For each element in the vector $p_i$, say $p_{ij}$\\
\begin{equation*}
    p_{ij} - p_{ij}^2 \leq \frac{1}{4}
\end{equation*}
So the error of simulation will be controlled within
\begin{equation*}
    \frac{1}{R} \times \frac{n}{4} = \frac{n}{4R}
\end{equation*}

\begin{figure}%[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{simu_accuracy.pdf}
    \caption{Simulation accuracy test result, the repeating time set as $10^4$}
\end{figure}

Figure 3 illustrates the performance of SA method. For a fixed m and t, as n increases MAE decreases. Due to sparsity effect if we fix $n$, we could find MAE also drops while m increasing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Time Efficiency}
In this section, we show computation efficiency of simulation method and MD-DFT for calculating all probability mass points as $(n,m)$ gets large. We generate random matrices with respect to given $(n, m)$, record the calculating time of each method. As for normal approach, it is a asymptotic method, we only need to calculate the asymptotic normal distribution.

\begin{table}%[h!]
\centering
\caption{Accuracy of simulation method.}\label{tab:my_label}
\begin{tabular}{c|c|c}
\hline
\hline
     $(n, m)$ & MD-DFT & Simulation based $(10^6)$  \\
\hline
    (4, 3) & 0.023 & 0.542 \\
\hline
    (6, 3) & 0.002 & 0.852\\
\hline
    (8, 3) & 0.003 & 1.177\\
    (10, 3) & 0.003 & 1.634\\
    (20, 3) & 0.014 & 4.532\\
    (50, 3) & 0.098 & 22.349\\
    (100, 3) & 0.49 & 80.243\\
    (200, 3) & 2.834 & 307.350\\
    (1000, 3) & 249.449  & 7992.507 \\
\hline\hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Study the computing of the three methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Recommendations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For small $m$ and moderate $n$, the MD-DFT can be used.

For large $m$ and moderate $n$, the simulation-based method can be used.

For large $n$, the NA-based method can be used.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Calculation of Voting Probability}


Do some example like this:

https://stats.stackexchange.com/questions/274211/calculating-the-probability-of-someone-winning-from-a-poll




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Statistical Inference for Aggregated Data}\label{sec:model.est.inf}	
First introduce the out "Logistic-like" model to fit a given aggregated dataset with selected features and a categorical response variable that has $m$ categories. By dividing the rows of our data into $H$ groups $G_1,\dots,G_{S}$, the group size of each group is $s_i,i=1,\dots,S$. Each $s_i$ are positive integer but not necessarily to be equal. Let the quantity for each category of group $G_i$ be $\boldsymbol{x}^{(i)} = (x_1^{(i)}, \dots, x_m^{(i)})'$, $m$ is the category number. Denote $\boldsymbol{H}^{(i)}$ to be the covariate matrix of $G_i$, then $\boldsymbol{H}^{(i)} = (\boldsymbol{1}, \boldsymbol{h}_{1}^{(i)},\dots,\boldsymbol{h}_{s_i}^{(i)})'$ is a $s_i \times v$ matrix with first column being $\boldsymbol{1}$, where $v$ equals to the number of covariates plus one. Let $\boldsymbol{P}^{(i)} = (p_{jk}^{(i)})$ be the SPM for group $G_i$, $i = 1, \dots, S$, $j = 1,\dots ,s_i$ and $k = 1,\dots, m$. Let the probability of getting $\boldsymbol{x}^{(i)}$ for group $G_i$ be $p(\boldsymbol{x}^{(i)})$. The total log-likelihood for all groups can be computed as
\begin{equation*}
    \ell = \sum_{i=1}^{S}\ell_i = \sum_{i=1}^{S}\log p(\boldsymbol{x}^{(i)})
\end{equation*}
Where $p(\boldsymbol{x}^{(i)})$ can be computed via Poisson Multinomial distribution with SPM $\boldsymbol{P}^{(i)}$.\\
Set category $m$ as baseline and use softmax function to form the $\boldsymbol{P}^{(i)}$ for each group through parameter $\boldsymbol{\beta} = (\boldsymbol{\beta}_1, \dots, \boldsymbol{\beta}_{m-1})$ as
\begin{align*}
    p_{j k}^{(i)} = \frac{\exp{\left(\boldsymbol{h}_{j}^{(i)} \boldsymbol{\beta}_{k}\right)}}{1 + \sum_{k=1}^{m-1}\exp{\left( \boldsymbol{h}_{j}^{(i)} \boldsymbol{\beta}_{k} \right)}} 
    \quad k \neq m \quad \text{and } \quad
    p_{i,m}^{(i)} = \frac{1}{1 + \sum_{k=1}^{m-1}\exp{\left( \boldsymbol{h}_{j}^{(i)} \boldsymbol{\beta}_{k} \right)}}
\end{align*}
Then we are able to estimate our parameters on the direction of minimizing total log-likelihood and finally get our estimate $\hat{\boldsymbol{P}}^{(i)}$. \\

In the rest of this part, we apply the model to the dataset "ai4i". The dataset "ai4i" is a synthetic machine failure dataset that reflects real predictive maintenance data encountered in industry. The data consists of 10000 products(rows) and 14 features(columns) including Product type, Air temperature, Process temperature, Rotational speed and others. \\

For demonstration purpose, here we only use the first 1000 rows and Product type as response variable, Air temperature and Process temperature as covariates. The feature Product type is categorical and has three levels, "M", "L", "H", we denote them as category 1, 2, 3 for simplicity. The number of products that fall in category 1, 2, and 3 are 285, 601, and 114. The other two features are continuous.
\\
We randomly divide the dataset into 100 groups $G_1,\dots,G_{100}$, the smallest group has size of three rows and the largest one has 18 rows. Note that readers can use other criterion to divide the dataset by their own. Notice the covariate number is three including intercept and the response has three categories, thus the dimension of our parameter matrix $\boldsymbol{\beta}$ will be $3 \times 2$ if we set category 3 as baseline.

The estimates of the parameters are
\begin{equation*}
\hat{\boldsymbol{\beta}} = 
\begin{pmatrix}
 1.07484986 & 2.2820922 \\
 1.62342045 & 1.9108976 \\
 -0.06277732 &-0.8455964
\end{pmatrix}
\end{equation*}
The corresponding $\hat{\boldsymbol{P}}^{(1)}$ and $\hat{\boldsymbol{P}}^{(5)}$ for group 1 and group 5 are
\begin{equation*}
    \hat{\boldsymbol{P}}^{(1)} = \begin{pmatrix}

 0.11914 & 0.63310 & 0.24776\\
 0.12326 & 0.57268 & 0.30406\\
 0.14809 & 0.47560 & 0.37631\\
 0.14504 & 0.56971 & 0.28525\\
 0.12451 & 0.54095 & 0.33454\\
 0.04170 & 0.55944 & 0.39886\\
 0.03559 & 0.53568 & 0.42873\\
 0.04890 & 0.54668 & 0.40442
    \end{pmatrix} \quad \text{, }
    \hat{\boldsymbol{P}}^{(5)} = \begin{pmatrix}
 0.15604 & 0.70766 & 0.13630\\
 0.14469 & 0.73976 & 0.11555\\
 0.11246 & 0.67372 & 0.21382\\
 0.12036 & 0.64885 & 0.23079\\
 0.11263 & 0.61304 & 0.27433\\
 0.11563 & 0.55029 & 0.33408\\
 0.11774 & 0.61672 & 0.26554\\
 0.15071 & 0.52510 & 0.32419\\
 0.13878 & 0.56645 & 0.29477\\
 0.08194 & 0.54561 & 0.37245\\
 0.06307 & 0.58191 & 0.35502
    \end{pmatrix}
\end{equation*}
For group 1 and 5, $\boldsymbol{x}^{(1)} = (1,5,2)'$ and $\boldsymbol{x}^{(5)} = (2,6,3)'$, so $p(\boldsymbol{x}^{(1)}) = 0.070$, $p(\boldsymbol{x}^{(5)}) = 0.067$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uncertainty Quantification in Classification}

Confusion matrix

Use Theorem for independent.

In machine learning classification problem with multiple labels, for each unit in the test set, the probability of the unit belongs to each class is computed. Usually, the predicted class is assigned as the highest probability. In the classifiers (i.e., soft classification), the unit class is randomly assigned according to the predicted probabilities, leading to randomness in the confusion matrix. The PMD can be used to characterize the distribution of the counts in the confusion matrix. 



Yueyao: reliability course project used EL images for classification. The confusion matrix is obtained (Table 4 of the course report). In some senses, Table 4 only provide the point estimates, with PMD the distribution of the counts in each cell can be found, and can be displayed by an array of histograms. 


A public dataset (\url{https://github.com/zae-bayern/elpv-dataset}) of solar cells extracted from high resolution EI images of monocrystalline and polycrystalline PV modules.


All images are normalized with respect to size and perspective. Additionally, any distortion induced by the camera lens used to capture the EL images was eliminated prior to solar cell extraction.

Then we will talk about how to process these data. They hired some evaluators and ask them two questions. The first is how do they think the function of the solar cells, functional or defective; the second is how they are confident about their assessments. Then we can get four different situations as Table \ref{tt}

\begin{table}[h!]
	\centering
	\caption{Partitioning of solar cells into functional and defective, with an additional self-assessment on the rater's confidence after visual inspection. Non-confident decisions obtain a weight lower than 100\% for the evaluation of the classifier performance }
	\begin{tabular}{c|c|c|c}
		\hline
		\hline
		Condition  & Confident?  &  Label $p$ & Weight $w$ \\
		\hline
		\multirow{2}{*}{functional} & \cmark & functional & 100\% \\
		& \xmark& defective  & 33\% \\
		\hline
		\multirow{2}{*}{defective} & \cmark & defective & 100\%  \\
		& \xmark & defective & 67\% \\
		\hline
		\hline
	\end{tabular}
	
	
	
	\label{tt}
	
\end{table}
According to the probability how likely the product is defective, we assign four labels to sollar cells, 0\%, 33.33\%, 66.66\%, 100\%.

We split our data to training data (80\%) and test data (20\%), then fitting a CNN model. In CNN model, we use Relu activation function and set the kernel size $3 \times 3$ and stride $1 \times 1$. We also allow our model to pad. After ten epochs, the model converges.

\begin{figure}
	\centering
	\includegraphics[scale=1.0]{figures/ConfusionHistogram.pdf}
	\caption{The histogram of each prediction cells.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Illustrations of the R Package}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Examples}
Illustrate the use of major functions in the R package.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Benchmark of R Packages for Poisson Binomial Distribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Areas for Future Research}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We develop algorithm that can be useful for computing the pmf of the PMD distribution, which is challenging to compute but useful in many application scenarios.





	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section*{Acknowledgments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{apacite}


\bibliographystyle{chicago}
\bibliography{refs}	

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

